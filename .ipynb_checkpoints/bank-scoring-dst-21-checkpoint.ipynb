{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт и настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "!pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# этот блок используется на kaggle\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "# PATH_to_file = '/kaggle/input/sf-dst-scoring/'\n",
    "\n",
    "# этот блок используется на локальной машине\n",
    "PATH_to_file = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File train.csv does not exist: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4f7b6cd0d54f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Предварительная загрузка и просмотр\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH_to_file\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File train.csv does not exist: 'train.csv'"
     ]
    }
   ],
   "source": [
    "# Предварительная загрузка и просмотр\n",
    "\n",
    "df = pd.read_csv(PATH_to_file+'train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем данные\n",
    "\n",
    "train = pd.read_csv(PATH_to_file + 'train.csv')\n",
    "test= pd.read_csv(PATH_to_file + 'test.csv')\n",
    "\n",
    "display(train.info())\n",
    "display(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"train: shape\" + str(train.shape), train.columns)\n",
    "display(\"test: shape\" +str(test.shape), test.columns)\n",
    "\n",
    "# Видим, что в тестовой выборке нет данных признака \"default\", что логично."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.isna().sum())\n",
    "display(test.isna().sum())\n",
    "\n",
    "# Мы видим, что и в тренировочной и в тестовом примере отсутствуют значения в признаке \"education\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем пометку, где тренировочная выборка, где тестовая\n",
    "train['Train'] = 1\n",
    "test['Train'] = 0\n",
    "\n",
    "#Заранее сохраняем значения client_id для предсказания тестовой выборки\n",
    "id_test = test['client_id']\n",
    "\n",
    "bank = train.append(test, sort=False).reset_index(drop=True) # объединяем датафреймы\n",
    "print(f'bank.shape = {bank.shape}')\n",
    "display(bank.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преобразование и чистка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank['education'].hist()\n",
    "# В датафрейме только признаке \"education\" есть пустые значения. \n",
    "# Количетсво пустых значений существенно меньше общего количества строк в таблице, \n",
    "# в данном признаке сильно превалирует значение \"SCH\", мы меняем значение на \"SCH\".\n",
    "\n",
    "bank['education'].fillna('SCH', inplace=True)\n",
    "\n",
    "# Замечание: нам не нужен признак \"client_id\", мы удалим его\n",
    "bank.drop('client_id', axis=1, inplace=True)\n",
    "print(f'bank.shape = {bank.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мы видим, что в отличие от всех других признаков, признак \"app_date\" в формате, не пригодном для дульнейшей работы.\n",
    "\n",
    "bank['app_date'] = pd.to_datetime(bank['app_date'], format='%d%b%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделение на категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нам необходимо разделить признаки на категории.\n",
    "display(bank.head(3))\n",
    "display(bank.nunique())\n",
    "\n",
    "# На основании представленной информации разделим признаки следующим образом\n",
    "num_features = ['age', 'decline_app_cnt', 'score_bki', 'bki_request_cnt', 'region_rating', 'income']\n",
    "cat_features = ['education', 'home_address', 'work_address', 'sna', 'first_time']\n",
    "bin_features = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']\n",
    "date_features = ['app_date']\n",
    "\n",
    "# Отдельно остается поле \"default\" - целевая переменная, которую необходимо бужет предсказать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание нового признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так как мы не можем работать с признаком \"app_date\" напрямую, создадим новый признак:\n",
    "# признак 'app_days_now', которые представляет собой разницу в днях от некоторого дня, \n",
    "# например от последнего указанного дня +30 дней (это первый весомый период по учету дефолта)\n",
    "\n",
    "max_date = bank['app_date'].max().date() + timedelta(days=30)\n",
    "bank['app_days_now'] = bank['app_date'].apply(lambda x: (max_date - x.date()).days)\n",
    "\n",
    "# Добавим данные переменные в числовые признаки\n",
    "num_features.extend(['app_days_now'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Корреляция и распределение признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассмотрим корреляцию признаков\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(bank[bank['Train']==1].corr().abs(), annot=True, fmt='.2f', cmap='PuBu')\n",
    "\n",
    "# Выводы: так как максимальная корреляция чуть превосходит 0.7, то никакие признаки удалять не будем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассмотрим как распределены числовые признаки\n",
    "\n",
    "fig, ax = plt.subplots(len(num_features), 2, figsize=(10, 40))\n",
    "\n",
    "\n",
    "#Изобразим графики значений и логарима значений\n",
    "\n",
    "for x in range(len(num_features)):\n",
    "    new_series_log = np.log(bank[bank['Train']==1][num_features[x]] + 1)\n",
    "    \n",
    "    ax[x, 0].hist(bank[bank['Train']==1][num_features[x]], rwidth=0.9, alpha=0.7)\n",
    "    ax[x, 0].set_title(num_features[x])\n",
    "    \n",
    "    ax[x, 1].hist(new_series_log, rwidth=0.9, alpha=0.7)\n",
    "    ax[x, 1].set_title('log of ' + num_features[x])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование числовых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На основании графиков можно сделать следующие преобразования:\n",
    "\n",
    "bank['age'] = np.log(bank['age'] + 1)\n",
    "bank['decline_app_cnt'] = np.log(bank['decline_app_cnt'] + 1)\n",
    "bank['bki_request_cnt'] = np.log(bank['bki_request_cnt'] + 1)\n",
    "bank['income'] = np.log(bank['income'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Изобразим графики распределения категориальных и бинарных признаков\n",
    "\n",
    "col_list = cat_features + bin_features\n",
    "\n",
    "plt.figure()\n",
    "for column in col_list:\n",
    "    plt.bar(bank[bank['Train']==1][column].unique(), bank[bank['Train']==1][column].value_counts(), width=0.5, alpha=0.7)\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "    \n",
    "# На основании графиков можно сделать следующие вывод:\n",
    "# В признаке \"education\" сильно превалирует значение \"SCH\"\n",
    "# В признаке \"work_address\" превалирует значение \"2\"\n",
    "# В признаке \"sna\" превалирует значение \"4\"\n",
    "# В признаке \"first_time\" превалирует значение \"1\"\n",
    "# В бинарных признаках только \"sex\" распределен достаточно равномерно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Представим таблицу описания для оценки, что количество выбросов указано корректно\n",
    "display(bank[bank['Train']==1][num_features].describe())\n",
    "\n",
    "outlier_dic = {}\n",
    "\n",
    "for column in num_features:\n",
    "    perc25 = percentile(bank[column], 25)\n",
    "    perc75 = percentile(bank[column], 75)\n",
    "    iqr = perc75 - perc25\n",
    "    low_range = perc25 - 1.5 * iqr\n",
    "    upper_range = perc75 + 1.5 * iqr\n",
    "    out_count = bank[bank['Train']==1][column].apply(lambda x: None if x < low_range or x > upper_range else x).isna().sum()\n",
    "    outlier_dic[column] = [round(low_range, 2), round(upper_range, 2), out_count]\n",
    "\n",
    "print('Результаты по выбросам:\\n')\n",
    "for key, val in outlier_dic.items():\n",
    "    print(f'{key}: ниж.граница = {val[0]}, верх.граница = {val[1]}, кол-во выбросов = {val[2]}')\n",
    "\n",
    "# По данным можно сделать следующие выводы:\n",
    "# Признак \"decline_app_cnt\" имеет очень много выбросов, подумаем, нужно ли удалять:\n",
    "#    Так как по распределению видно, что сильно преобладает занчение 0,\n",
    "#    то удаление выбросов приведет к тому, что нужно будет удалить весь столбец.\n",
    "# Признаки \"score_bki\", \"bki_request_cnt\", \"income\"  имеют мало выбросов, удалять не будем.\n",
    "# Признак \"region_rating\" имеет очень много выбросов, подумаем, нужно ли удалять. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Значимость переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Далее нам надо будет рассмотреть значимость числовы и бинарных переменных. \n",
    "# Для этого сделаем преобразование бинарных переменных.\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in bin_features:\n",
    "    bank[column] = label_encoder.fit_transform(bank[column])\n",
    "\n",
    "imp_num = pd.Series(f_classif(bank[bank['Train']==1][bin_features + num_features], \n",
    "                              bank[bank['Train']==1]['default'])[0], index=bin_features + num_features)\n",
    "imp_num.sort_values(inplace = True)\n",
    "imp_num.plot(kind = 'barh')\n",
    "\n",
    "# Мы видим, что самым значимым является признак \"scoke_bki\" (сильно важнее других), \n",
    "# далее идет \"decline_app_cnt\", \"region_rating\" и т.д., последним идет \"sex\".\n",
    "# Учитывая значимость признаков \"decline_app_cnt\", \"region_rating\" и их данные по выбросам, выбросы удалять не будем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь рассмотрим значимость категориальных переменных\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "bank['education'] = label_encoder.fit_transform(bank['education'])\n",
    "\n",
    "imp_cat = pd.Series(mutual_info_classif(bank[bank['Train']==1][cat_features], \n",
    "                                        bank[bank['Train']==1]['default'], discrete_features = True), index=cat_features)\n",
    "imp_cat.sort_values(inplace = True)\n",
    "imp_cat.plot(kind = 'barh')\n",
    "\n",
    "# Мы видим, что самым значимым является признак \"sna\", а самым незначимым \"work_address\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Подготовка данных для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делим выборку обратно на тренировочную и тестовую\n",
    "bank_train = bank[bank['Train']==1]\n",
    "bank_test = bank[bank['Train']==0]\n",
    "\n",
    "# Категориальные признаки преобразовываем\n",
    "X_cat_train = pd.get_dummies(bank_train[cat_features], columns=cat_features).values\n",
    "X_cat_test = pd.get_dummies(bank_test[cat_features], columns=cat_features).values\n",
    "\n",
    "# Стандартизуем числовые признаки\n",
    "X_num_train = StandardScaler().fit_transform(bank_train[num_features].values)\n",
    "X_num_test = StandardScaler().fit_transform(bank_test[num_features].values)\n",
    "\n",
    "# Бинарные признаки\n",
    "X_bin_train = bank_train[bin_features].values\n",
    "X_bin_test = bank_test[bin_features].values\n",
    "\n",
    "\n",
    "# Объединяем данные\n",
    "X = np.hstack([X_cat_train, X_num_train, X_bin_train])\n",
    "Y = bank_train['default'].values\n",
    "test_val = np.hstack([X_cat_test, X_num_test, X_bin_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Далее будем использовать параметры регуляризации для улучшения модели\n",
    "# Регуляризация. Подбор параметров\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=RANDOM_SEED, shuffle = True)\n",
    "\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Зададим ограничения для параметра регуляризации\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "penalty = ['l1', 'l2']\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "clf = GridSearchCV(model, hyperparameters, cv=5, verbose=0)\n",
    "best_model = clf.fit(X_train, Y_train)\n",
    "\n",
    "print('Лучший penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Лучшее C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строим модель и прорисовываем ROC-кривую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l2', C=1.0, max_iter=500)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred)\n",
    "roc_auc_val = roc_auc_score(Y_test, Y_pred)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([0, 1], label='Baseline', linestyle='--')\n",
    "plt.plot(fpr, tpr, label = 'Regression')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title(f'Logistic Regression ROC AUC = {roc_auc_val:.4f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_submis = LogisticRegression(penalty='l2', C=1.0, max_iter=1000)\n",
    "model_submis.fit(X, Y)\n",
    "prob_submis = model_submis.predict_proba(test_val)[:,1]\n",
    "\n",
    "submission = pd.DataFrame({'client_id': id_test, 'default': prob_submis})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
